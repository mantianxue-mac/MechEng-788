{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment_1.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1zBIqK6rE2rkyPLC1EVb0qIrbGTMhzSlj","authorship_tag":"ABX9TyPnzQNlT8jcqhwzvBKbfaZG"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"oEbZ4i7rehw9"},"source":["# Assignment 1:\n","\n"," Given the most popular architecture of the neural networks (https://www.asimovinstitute.org/neural-network-zoo/ ) , select 5 architectures and elaborate on the following questions:\n","\n","-          What is so special about them (benefit)\n","\n","-          What could be the use cases for the selected architecture (you may find the https://paperswithcode.com/sota handy)\n","\n","-          The limits of the architecture\n","\n","## Feed Forward Neural Networks (FF)\n","\n","![FF](https://images.deepai.org/django-summernote/2019-06-06/5c17d9c2-0ad4-474c-be8d-d6ae9b094e74.png)\n","\n","It was proposed in 1950s, it is the first model of AI. This model can be used to model the relationship between input and output. The application of such network includes:\n","- Function approximation\n","- Pattern classification\n","- Object recognition\n","- Data compression\n","\n","The limitation:\n","- Depends on available data\n","- unexplainable\n","\n","\n","## Recurrent Neural Networks\n","![RNN](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/1024px-Recurrent_neural_network_unfold.svg.png)\n","\n","The middle(hidden) layer is connected to an additonal context units fixed with a weight of one. The fixed back-connection save a copy of the previous values of hidden units in the context units. Thus the network can maintain a sort of state, allowing it to perform sequence-prediction that beyond the power of a standard multilayer perceptron.\n","\n","Elman network\n","$$\n","\\begin{aligned}\n","h_t &= \\sigma_h(W_hx_t + U_hh_{t-1}+b_h) \\\\\n","y_t &= \\sigma_y(W_yh_t + b_y) \n","\\end{aligned}\n","$$\n","\n","RNN is able to deal with sequential data, its application includes machine translation, predictions, handwriting recognition, speech recognition..\n","\n","The disadvantages of RNN:\n","- Slow computation due to recurrent state\n","- Training can be difficult\n","- Gradient vanishing/exploding\n","\n","## Long Short Term Memory (LSTM)\n","![LSTM](https://developer.nvidia.com/sites/default/files/pictures/2018/lstm.png)\n","\n","\n","LSTM consists of units with self-connection having a constant weight of 1.0. This allows a value or gradient flow into this unit to be preserved and subsequenlty retrieved at some required time step. This memory cell is capable of storing information in the past. Also, inspired from the gate circuit, LSTM uses the following gate equations to handle gradient vanish/explodes problem.\n","\n","$$\n","\\begin{aligned}\n","  i_t &= \\sigma(w_i[h_{t-1},x_t] + b_i) \\\\\n","  f_t &= \\sigma(w_f[h_{t-1},x_t] + b_f) \\\\\n","  o_t &= \\sigma(w_o[h_{t-1},x_t] + b_o)\n","\\end{aligned}\n","$$\n","where $\\sigma $ is sigmoid function\n","\n","LSTMs have been shown to be able to learn complex sequences, such as writing like Shakespeare or composing primitive music. \n","\n","According to The following limitations\n","- Difficult to solve \"strongly delayed XOR problems\"\n","- Each memory cell block requires two additional units, which leads to more weights and complexity of the network compare to standard recurrent nets\n","- LSTM runs into problems similar to those of feedforward nets seeing the entire input string at once\n","\n","## Convolutional Neural Networks (CNN)\n","![CNN](https://www.researchgate.net/profile/Alejandro_Baldominos/publication/322079459/figure/fig1/AS:576480327331840@1514455028480/Typical-structure-of-a-sequential-convolutional-neural-network.png)\n","\n","Convolution nets combine local receptive fields, shared weights, and spatial sub-sampling such that it can deal with some degrees of shift, scale, and distortion.\n","\n","Convolution nets first deployed commerically in 1993 on a DSP board in check-reading ATM machines in Europe and US, and later in large bank check reading machines. This motivated MicroSoft deploy ConvNets in OCR and handwritting recognition. It also been used in object detection in images, including faces with record accuracy and real-time performance. It also have beed used for vision-based obstacle avoidance for off-road mobile robots.\n","\n","[Geoffrey Hinton gave talks on the limitation of ConvNets](https://www.youtube.com/watch?v=Jv1VDdI4vy4&feature=youtu.be&fbclid=IwAR0nhgq-sANwu13LKa8NrMkh7VslVlLjB6fO4GQ0wQnUDULhWrDyzPuwyek&t=1s&ab_channel=FieldsInstitute)\n","\n","According to Hinton, the ConvNets has following limits:\n","- Difference between human perception and ConvNets, it doesn't handle well the view point change. The current solution is provide massive amount of image showing objects in different positions, and hoping the ConvNets will see enough to have better generalization ability.\n","- Pooling gives a small amount of translational invariance at each level\n","- Pooling reduces the number of inputs to the next layer of feature extraction\n","- ... many more\n","\n","## Generative Adversarial Networks(GAN)\n","GANs consist of any two networks, with one taksed to generate content and the other has to judge content.\n","\n","![GAN](https://paperswithcode.com/media/methods/gan.jpeg)\n","\n","To learn the generator's distribution $p_g$ over data $x$, define a prior on input noise variables $p_z(z)$, then a mapping to data space is represented by $G(z;\\theta_g)$, where $G$ is a differentiable function represented by a MLP with params $\\theta_g$. The second network is defined outputs a single scalar. $D(x)$ represents the probability that $x$ came from the data rather than $p_g$. $D$ is trained to maximize the probability of assigning the correct label to both training example and samples from $G$. Simultaneously, $G$ is trained to minimize $\\log(1-D(G(z)))$\n","\n","According to Goodfellow (GAN's creator), GAN has the disadvantages that there is no explicit representation of $p_g(x)$, and that $D$ must be synchronized well with $G$ during training. The negative chains of a Boltzmann machine must be kept up to date between learning steps.\n","\n","GAN can be used in the following areas:\n","- Generate Examples for Image Datasets\n","- Generate Photographs of Human Faces\n","- Generate Realistic Photographs\n","- Generate Cartoon Characters\n","- Image-to-Image Translation\n","- Text-to-Image Translation\n","- Semantic-Image-to-Photo Translation\n","- Face Frontal View Generation\n","- Generate New Human Poses\n","- Photos to Emojis\n","\n","### GAN-generated faces\n","![GAN application](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/06/Examples-of-Photorealistic-GAN-Generated-Faces.png)\n","\n","### Video Prediction\n","![GAN application](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/06/Example-of-Video-Frames-Generated-with-a-GAN.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NTIUeV0GIG5G"},"source":["# Assignment 2:\n","\n","Data set: lab_results.csv\n","\n","Using the lab example (as the work flow), please design and develop a neural network to predict the outcome based on other independent parameters.\n","\n","- What is your strategy for designing the number of neurons and the number of hidden layer?\n","\n","- How to optimize the weights\n","\n","- Your choice of loss functions and optimizer\n","\n","## Strategy\n","Depending on the dataset size. Big dataset can have more hidden layers and neurons because the unknown weights can be optimized using the provided data. Small dataset should have small amount of neurons and hidden layers. Often, we can choose go deeper since the activiation function introduce more nonlinearity to the network while adding more neurons is just adding more linear combinations of activation function.\n","\n","## How to optimize the weights\n","Define proper loss function. Use the dataset provided input calculate the forward pass, find the error between the target and actual outputs. Use backpropagation to get the derivative of the loss function with respect to the weights of the network.\n","\n","## Choice of loss function and optmizer\n","Loss function:\n","- neg log-likelihood\n","- cross entropy\n","\n","Optimizer:\n","- sgd\n","- adam"]},{"cell_type":"code","metadata":{"id":"F6ovHysBJT5a","executionInfo":{"status":"ok","timestamp":1601220235301,"user_tz":-480,"elapsed":9676,"user":{"displayName":"Max Xue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4pp5gxHuE9fi6UFKbUVmN3rRxCqBbcDgxHdSmYg=s64","userId":"11327524951124945339"}},"outputId":"b6e113cd-15ee-4674-924f-c744e20a908f","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":93}},"source":["import pandas as pd\n","from google.colab import files\n","\n","uploaded = files.upload()\n","\n","import io\n","lab_results = pd.read_csv(io.BytesIO(uploaded['lab_results.csv']))"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-26a69593-7e37-49d8-bacd-55c11edb5244\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-26a69593-7e37-49d8-bacd-55c11edb5244\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving lab_results.csv to lab_results (1).csv\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(768, 9)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"UExViz0dKsqp","executionInfo":{"status":"ok","timestamp":1601227292857,"user_tz":-480,"elapsed":41580,"user":{"displayName":"Max Xue","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj4pp5gxHuE9fi6UFKbUVmN3rRxCqBbcDgxHdSmYg=s64","userId":"11327524951124945339"}},"outputId":"4fe07600-ca5a-4ecc-f83e-980a07abecc6","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# prepare training data\n","data = lab_results.to_numpy()\n","data.transpose()\n","train_inputs = data[:500,:-2]\n","train_lable = data[:500,-1]\n","test_inputs = data[500:,:-2]\n","test_lable = data[500:,-1]\n","print(f\"Feature Shape:{len(train_inputs)}\")\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.utils import to_categorical\n","\n","model = keras.Sequential(\n","    [\n","     layers.Dense(7, activation=\"relu\",name=\"input\",input_shape=(7,)),\n","     layers.Dense(10,activation=\"relu\",name=\"hidden1\"),\n","     layers.Dense(1,activation=\"tanh\",name=\"output\"),\n","    ]\n",")\n","\n","model.compile(\n","    optimizer='sgd',\n","    loss='binary_crossentropy',\n","    metrics=['accuracy'],\n",")\n","\n","model.fit(\n","    train_inputs,\n","    to_categorical(train_lable),\n","    epochs=100,\n","    batch_size=1,\n",")\n","\n","model.evaluate(test_inputs,to_categorical(test_lable))\n","\n","prediction = model.predict(test_inputs[:3])\n","\n","# Print our model's predictions.\n","print(np.argmax(prediction, axis=1)) \n","\n","print(test_lable[:3])"],"execution_count":70,"outputs":[{"output_type":"stream","text":["Feature Shape:500\n","Epoch 1/100\n","500/500 [==============================] - 0s 797us/step - loss: 0.7062 - accuracy: 0.5000\n","Epoch 2/100\n","500/500 [==============================] - 0s 798us/step - loss: 0.6938 - accuracy: 0.5000\n","Epoch 3/100\n","500/500 [==============================] - 0s 776us/step - loss: 0.6937 - accuracy: 0.5000\n","Epoch 4/100\n","500/500 [==============================] - 0s 796us/step - loss: 0.6936 - accuracy: 0.5000\n","Epoch 5/100\n","500/500 [==============================] - 0s 800us/step - loss: 0.6936 - accuracy: 0.5000\n","Epoch 6/100\n","500/500 [==============================] - 0s 780us/step - loss: 0.6935 - accuracy: 0.5000\n","Epoch 7/100\n","500/500 [==============================] - 0s 774us/step - loss: 0.6935 - accuracy: 0.5000\n","Epoch 8/100\n","500/500 [==============================] - 0s 795us/step - loss: 0.6935 - accuracy: 0.5000\n","Epoch 9/100\n","500/500 [==============================] - 0s 805us/step - loss: 0.6935 - accuracy: 0.5000\n","Epoch 10/100\n","500/500 [==============================] - 0s 788us/step - loss: 0.6934 - accuracy: 0.5000\n","Epoch 11/100\n","500/500 [==============================] - 0s 780us/step - loss: 0.6934 - accuracy: 0.5000\n","Epoch 12/100\n","500/500 [==============================] - 0s 771us/step - loss: 0.6934 - accuracy: 0.5000\n","Epoch 13/100\n","500/500 [==============================] - 0s 803us/step - loss: 0.6934 - accuracy: 0.5000\n","Epoch 14/100\n","500/500 [==============================] - 0s 780us/step - loss: 0.6934 - accuracy: 0.5000\n","Epoch 15/100\n","500/500 [==============================] - 0s 853us/step - loss: 0.6934 - accuracy: 0.5000\n","Epoch 16/100\n","500/500 [==============================] - 0s 777us/step - loss: 0.6934 - accuracy: 0.5000\n","Epoch 17/100\n","500/500 [==============================] - 0s 804us/step - loss: 0.6934 - accuracy: 0.5000\n","Epoch 18/100\n","500/500 [==============================] - 0s 786us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 19/100\n","500/500 [==============================] - 0s 769us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 20/100\n","500/500 [==============================] - 0s 782us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 21/100\n","500/500 [==============================] - 0s 804us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 22/100\n","500/500 [==============================] - 0s 781us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 23/100\n","500/500 [==============================] - 0s 805us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 24/100\n","500/500 [==============================] - 0s 762us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 25/100\n","500/500 [==============================] - 0s 788us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 26/100\n","500/500 [==============================] - 0s 782us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 27/100\n","500/500 [==============================] - 0s 768us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 28/100\n","500/500 [==============================] - 0s 817us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 29/100\n","500/500 [==============================] - 0s 765us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 30/100\n","500/500 [==============================] - 0s 774us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 31/100\n","500/500 [==============================] - 0s 801us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 32/100\n","500/500 [==============================] - 0s 787us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 33/100\n","500/500 [==============================] - 0s 785us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 34/100\n","500/500 [==============================] - 0s 798us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 35/100\n","500/500 [==============================] - 0s 776us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 36/100\n","500/500 [==============================] - 0s 772us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 37/100\n","500/500 [==============================] - 0s 786us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 38/100\n","500/500 [==============================] - 0s 823us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 39/100\n","500/500 [==============================] - 0s 773us/step - loss: 0.6933 - accuracy: 0.5000\n","Epoch 40/100\n","500/500 [==============================] - 0s 779us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 41/100\n","500/500 [==============================] - 0s 792us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 42/100\n","500/500 [==============================] - 0s 790us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 43/100\n","500/500 [==============================] - 0s 794us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 44/100\n","500/500 [==============================] - 0s 782us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 45/100\n","500/500 [==============================] - 0s 772us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 46/100\n","500/500 [==============================] - 0s 801us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 47/100\n","500/500 [==============================] - 0s 760us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 48/100\n","500/500 [==============================] - 0s 795us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 49/100\n","500/500 [==============================] - 0s 772us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 50/100\n","500/500 [==============================] - 0s 767us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 51/100\n","500/500 [==============================] - 0s 796us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 52/100\n","500/500 [==============================] - 0s 779us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 53/100\n","500/500 [==============================] - 0s 831us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 54/100\n","500/500 [==============================] - 0s 785us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 55/100\n","500/500 [==============================] - 0s 772us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 56/100\n","500/500 [==============================] - 0s 784us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 57/100\n","500/500 [==============================] - 0s 772us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 58/100\n","500/500 [==============================] - 0s 776us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 59/100\n","500/500 [==============================] - 0s 793us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 60/100\n","500/500 [==============================] - 0s 777us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 61/100\n","500/500 [==============================] - 0s 791us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 62/100\n","500/500 [==============================] - 0s 803us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 63/100\n","500/500 [==============================] - 0s 766us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 64/100\n","500/500 [==============================] - 0s 828us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 65/100\n","500/500 [==============================] - 0s 779us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 66/100\n","500/500 [==============================] - 0s 786us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 67/100\n","500/500 [==============================] - 0s 817us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 68/100\n","500/500 [==============================] - 0s 809us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 69/100\n","500/500 [==============================] - 0s 788us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 70/100\n","500/500 [==============================] - 0s 774us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 71/100\n","500/500 [==============================] - 0s 778us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 72/100\n","500/500 [==============================] - 0s 760us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 73/100\n","500/500 [==============================] - 0s 774us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 74/100\n","500/500 [==============================] - 0s 800us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 75/100\n","500/500 [==============================] - 0s 774us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 76/100\n","500/500 [==============================] - 0s 818us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 77/100\n","500/500 [==============================] - 0s 761us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 78/100\n","500/500 [==============================] - 0s 777us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 79/100\n","500/500 [==============================] - 0s 786us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 80/100\n","500/500 [==============================] - 0s 787us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 81/100\n","500/500 [==============================] - 0s 781us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 82/100\n","500/500 [==============================] - 0s 772us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 83/100\n","500/500 [==============================] - 0s 780us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 84/100\n","500/500 [==============================] - 0s 834us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 85/100\n","500/500 [==============================] - 0s 784us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 86/100\n","500/500 [==============================] - 0s 783us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 87/100\n","500/500 [==============================] - 0s 792us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 88/100\n","500/500 [==============================] - 0s 769us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 89/100\n","500/500 [==============================] - 0s 785us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 90/100\n","500/500 [==============================] - 0s 772us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 91/100\n","500/500 [==============================] - 0s 765us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 92/100\n","500/500 [==============================] - 0s 778us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 93/100\n","500/500 [==============================] - 0s 836us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 94/100\n","500/500 [==============================] - 0s 779us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 95/100\n","500/500 [==============================] - 0s 793us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 96/100\n","500/500 [==============================] - 0s 780us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 97/100\n","500/500 [==============================] - 0s 796us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 98/100\n","500/500 [==============================] - 0s 795us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 99/100\n","500/500 [==============================] - 0s 820us/step - loss: 0.6932 - accuracy: 0.5000\n","Epoch 100/100\n","500/500 [==============================] - 0s 765us/step - loss: 0.6932 - accuracy: 0.5000\n","9/9 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.5000\n","WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3ecf639510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","[0 0 0]\n","[0. 0. 1.]\n"],"name":"stdout"}]}]}